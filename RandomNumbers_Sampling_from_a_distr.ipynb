{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RandomNumbers_Sampling_from_a_distr.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLBAkL73NH/dUkbweOoK78",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vskokov/py525/blob/main/RandomNumbers_Sampling_from_a_distr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNjLcSad5awO"
      },
      "source": [
        "# Direct Sampling\n",
        "\n",
        "**Problem 1**\n",
        "\n",
        "For some specific pdf it is easy to obtain random numbers using the direct sampling. \n",
        "For example consider two uniformly distributed numbers $x_1$ and $x_2$. \n",
        "The new number $y = {\\rm max}(x_1,x_2)$ is distributed with  the pdf: \n",
        "\\begin{equation}\n",
        "  p(y)  = 2 y\\,. \n",
        "\\end{equation}\n",
        "To prove this consider pdf for $y$ \n",
        "\\begin{equation}\n",
        "  p(y)  = \\int dx_1 \\int  d x_2  p_u(x_1) p_u(x_2) \\delta(  y -   {\\rm max}(x_1,x_2) )  =\n",
        "  \\int_0^1 dx_1 \\int_0^1 d x_2  \\delta(  y -   {\\rm max}(x_1,x_2) )  \\,.\n",
        "\\end{equation}\n",
        "Now splitting the integral over $x_2$ in two \n",
        "\\begin{equation}\n",
        "\\int_0^1 d x_2 \\delta(  y -   {\\rm max}(x_1,x_2) )  = \\left( \\int_0^{x_1} d x_2 \\delta(  y -   x_1 )   \\right) + \\left( \\int_{x_1}^1 d x_2 \\delta(  y -   x_2 )   \\right),\n",
        "\\end{equation} \n",
        "we obtain $p(y) = 2 y$. \n",
        "\n",
        "Similarly, to generate random numbers $z$ which follow the pdf \n",
        "\\begin{equation}\n",
        "p(z)  = k z^{k-1} \n",
        "\\end{equation}\n",
        "it is sufficient to consider \n",
        "\\begin{equation}\n",
        "z = {\\rm max} (x_1, x_2, \\dots, x_k)\\,\n",
        "\\end{equation}\n",
        "with all $x_k$ uniformly distributed over $[0,1]$. \n",
        "\n",
        "**Write code to generate the distribution $p(x) = 3 x^2$. Plot the histrogram for the generated random numbers and compare it to the actual distribution.**\n",
        "\n",
        "\n",
        "To genrate uniformly distributed numbers use\n",
        "\n",
        "```\n",
        "from random import *\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = [uniform(0,1) for i in range(10000)]\n",
        "```\n",
        "\n",
        "Plotting your generated distributions (here is the uniform distribution):\n",
        "\n",
        "```\n",
        "plt.hist(x, density = True,bins = 50)\n",
        "plt.plot([0,1],[1,1]) #analytical distribution\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Problem 2**\n",
        "\n",
        "There is an elegant method which can be used to generate  random numbers which follow a normal distribution: \n",
        "\\begin{equation}\n",
        "p(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( - \\frac{z^2} { 2}\\right)\\,.\n",
        "\\end{equation}\n",
        "For this take two uniformly distributed $x_1$ and $x_2$ and construct \n",
        "\\begin{align}\n",
        "z_1 &= \\cos \\left(2 \\pi x_2\\right) \\sqrt{-2 \\ln x_1}, \\\\ \n",
        "z_2 &= \\sin \\left(2 \\pi x_2\\right) \\sqrt{-2 \\ln x_1}\n",
        "\\end{align}\n",
        "**Write code to generate the normal distribution. Plot the histrogram for the generated random numbers and compare it to the actual distribution.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7y1MBCr5ZZ9"
      },
      "source": [
        "# Code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afeVxBtFAkQo"
      },
      "source": [
        "# Inverse transformation\n",
        "\n",
        "**Problem 3**\n",
        " \n",
        "Let $p(x)$, $x\\in [x_{\\rm min},x_{\\rm max}]$ denote the pdf from which we want to obtain our random numbers. The corresponding cumulative distribution function (cdf) is \n",
        "\\begin{equation}\n",
        "P(x)  = \\int_{x_{\\rm min}}^x dx' p(x'). \n",
        " \\end{equation}\n",
        " The cdf is monotonically increasing and $P(x_{\\rm max}) = 1$.  Consider the \n",
        " source $y$ distributed uniformly in the interval $[0,1]$.  From the conservation of the probability \n",
        " \\begin{equation}\n",
        " p_u(y) dy = p(x) dx\n",
        " \\end{equation} \n",
        " or \n",
        "  \\begin{equation}\n",
        " p_u(y) = p(x) \\left(\\frac{dy}{dx}\\right)^{-1}\\,.\n",
        " \\end{equation} \n",
        " This equation can be solved by the choice $y=P(x)$, since, in this case, we know that $\\frac{dy}{dx} = p(x)$. We thus arrive at \n",
        "  \\begin{equation}\n",
        " x = P^{-1}(y), \n",
        "  \\end{equation}\n",
        " where $P^{-1}$ is the inverse function of $P$. It is an obvious limitation of this method that it requires the inverse  $P^{-1}(y)$ to exist and that $P(x)$ must be calculated and inverted analytically.\n",
        "\n",
        " As en example let consider **the exponential distribution**:\n",
        " \\begin{equation}\n",
        " p(x)  = \\frac{1}{\\lambda} \\exp \\left( - \\frac{x}{\\lambda} \\right)\\,, \n",
        " \\end{equation}\n",
        " where $\\lambda>0$ and $x\\in[0,1]$. \n",
        " The cdf is \n",
        "  \\begin{equation}\n",
        " P(x)  = 1 - \\exp \\left( - \\frac{x}{\\lambda}  \\right)\\,\n",
        " \\end{equation}\n",
        " and therefore the required transformation is \n",
        "  \\begin{equation}\n",
        " x = -\\lambda \\ln (1-y) \\,.\n",
        "  \\end{equation}\n",
        " Since the uniform distribution is symmetric, we can simplify the above expression:  \n",
        "   \\begin{equation}\n",
        " x = -\\lambda \\ln y \\,.\n",
        "  \\end{equation}\n",
        "\n",
        "**Write code to generate the distribution for $\\lambda=0.5$. Plot histogram and compare to the analytical distribution.** \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UlJ84vxBiSB"
      },
      "source": [
        "#Code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i5Vi1bjBlts"
      },
      "source": [
        "# Rejection method \n",
        "\n",
        "The rejection method is particularly suitable if the inverse transformation method\n",
        "fails. One of the most prominent versions of the rejection method is the\n",
        "Metropolis algorithm.\n",
        "\n",
        "The basic idea of the rejection method is to draw random numbers $x$ from another,\n",
        "preferably analytically invertible pdf $h(x)$ and check whether or not they lie within\n",
        "the desired pdf $p(x)$. If this is the case the random number $x$ is accepted, otherwise it will be rejected. \n",
        "\n",
        "Let $p(x)$ denote the pdf from which we want\n",
        "to draw random numbers. Let $h(x)$ be another pdf, which can easily\n",
        "be sampled and which is chosen in a such a way that the inequality \n",
        "\\begin{equation}\n",
        "p(x) \\le c h(x)\n",
        "\\end{equation}\n",
        "holds for all $x\\in [x_{\\rm min},x_{\\rm max}]$, where $c$ is some constant $c\\ge 1$. \n",
        "Often the function $c h (x)$ is referred to as the envelope of $p(x)$ within the given interval. The strategy is to \n",
        "sample a random variable $x_t$ (trial state) from $h(x)$ and accept it with\n",
        "probability $\\frac{p(x)}{c\\,  h(x)}$. \n",
        "\n",
        "To show that this procedure indeed leads to the pdf $p(x)$, consider $p(A|x)$ denoting the probability that a given value is accepted and $g(x)$ denoting the probability that we produce a variable $x$ with the help of this algorithm. Furthermore, let's define $P(x=x')$, which  stands for the probability that a trial state $x'$ is generated. Thus \n",
        "\\begin{equation}\n",
        "g(x) \\propto  P(x=x') p(A|x)  = h(x') \\frac{ p(x') } { c h(x')} \\propto p(x')\n",
        "\\end{equation} \n",
        "\n",
        "The probability that an arbitrary trial state $x'$ is accepted can be obtained in the following way: \n",
        "\\begin{align}\n",
        "    P(A) = \\int dx' p(A|x') P(x=x') = \\int dx' \\frac{p(x')}{c h(x')} h(x') = \\frac{1}{c} \\int dx' p(x') = \\frac{1}{c}\\,. \n",
        "\\end{align}\n",
        "For a $d$-dimensional random variable the probability to accept it will be $c^{-d}$. We thus conclude that the bigger $c$ the worse is the acceptance probability of the rejection method, that is the less is the computational efficiency of the method. It is therefore important to choose the envelope $h(x)$ very carefully.\n",
        "\n",
        "**We will consider sampling  the normal distribution:**\n",
        "\\begin{align}\n",
        "    p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left( - \\frac{x^2}{2\\sigma^2} \\right). \n",
        "\\end{align}\n",
        "First of all lets consider only positive $x$ and thus modify the pdf \n",
        "\\begin{align}\n",
        "    q(x) = \\frac{\\sqrt{2}}{ \\sqrt{\\pi \\sigma^2}} \\exp \\left( - \\frac{x^2}{2\\sigma^2} \\right), \\quad x\\in [0,\\infty), \n",
        "\\end{align}\n",
        "where the normalization was adjusted. \n",
        "As an envelope we will use the exponential distribution, which can be easily sampled using the inverse transformation method\n",
        " \\begin{equation}\n",
        " h(x)  = \\frac{1}{\\lambda} \\exp \\left( - \\frac{x}{\\lambda} \\right)\\,. \n",
        " \\end{equation}\n",
        " We fix $\\lambda$ and $c$ to maximize the acceptance probability under the constraint \n",
        " \\begin{align}\n",
        "     q(x) \\le c h(x)\\,. \n",
        " \\end{align}\n",
        " This is equivalent to finding a maximal $q(x)/h(x)$. The corresponding value of $x_{\\rm opt}$ will define $c_{\\rm min} = q(x_{\\rm opt})/h(x_{\\rm opt})$. \n",
        " We have\n",
        " \\begin{align}\n",
        "     \\frac{d}{dx} \\left( \\frac{q(x)}{h(x)}  \\right) = \\sqrt{\\frac{2\\lambda^2}{\\pi \\sigma^2}} \\exp\\left ( \\frac{x}{\\lambda} - \\frac{x^2}{2\\sigma^2} \\right) \\left(  \\frac{1}{\\lambda} - \\frac{x}{\\sigma^2}  \\right)\\,.\n",
        " \\end{align}\n",
        " Setting this to 0 and solving for $x$, we obtain \n",
        " \\begin{align}\n",
        "     x_{\\rm opt} = \\frac{\\sigma^2}{\\lambda}\\,.\n",
        " \\end{align}\n",
        " Therefore we have \n",
        " \\begin{align}\n",
        "     c_{\\rm min} = \\frac{q(x_{\\rm opt})}{h(x_{\\rm opt})} = \\sqrt{\\frac{2\\lambda^2}{\\pi \\sigma^2}} \\exp\\left (\\frac{\\sigma^2}{2\\lambda^2} \\right)\\,. \n",
        " \\end{align}\n",
        " This relation defines $c_{\\rm min}$ for an arbitrary $\\lambda$. We can choose $\\lambda$ to minimize $c_{\\rm min}$: \n",
        " \\begin{align}\n",
        "     \\frac{d}{d\\lambda} c_{\\rm min} = 0 \\,.\n",
        " \\end{align}\n",
        " This is equivalent to \n",
        " \\begin{align}\n",
        "     1 - \\frac{\\sigma^2}{\\lambda^2} = 0 \\,, \n",
        " \\end{align}\n",
        " which leads to $\\lambda_{\\rm opt} = \\sigma$ and finally to \n",
        " \\begin{align}\n",
        "     c_{\\rm min} =  \\sqrt{\\frac{2\\lambda_{\\rm opt}^2}{\\pi \\sigma^2}} \\exp\\left (\\frac{\\sigma^2}{2\\lambda_{\\rm opt}^2} \\right) \n",
        "     =  \\sqrt{\\frac{2e}{\\pi}}\\,.\n",
        " \\end{align}\n",
        " The inverse of $c_{\\rm min}$ is approximately $0.76$. That is the acceptance probability will be about $76\\%$, which means that from four generated numbers three will be accepted.  \n",
        " \n",
        " The algorithmic implementation is as follows: \n",
        "\n",
        "1. Generate a uniform random number $y \\in [0,1]$\n",
        "2. Calculate $x' = - \\lambda_{\\rm opt} \\ln y$ with $\\lambda_{\\rm opt} = \\sigma$\n",
        "3. Draw a uniformly distributed random number $r\\in [0,1]$. If $r \\le q(x')/[c_{\\min} h(x')]$, then we accept the random number $x = x'$. Otherwise the number is rejected and we return to step 1. \n",
        "4. We draw another  a uniformly distributed random number $r\\in [0,1]$. If $r<0.5$ we set $x \\to -x$. \n",
        "5. We repeat the steps until we generate the desired number of random $x$. \n",
        " \n",
        " **Write code to generate the distribution for $\\sigma=0.5$. Plot histogram and compare to the analytical distribution.** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhLDFWESCcf3"
      },
      "source": [
        "#Code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZRdSwdqCe9q"
      },
      "source": [
        "# Metropolis \n",
        "\n",
        "The Metropolis algorithm is a more sophisticated method to produce random\n",
        "numbers from given distributions.\n",
        "\n",
        "The  algorithm is particularly useful to treat problems in statistical\n",
        "physics where thermodynamic expectation values of some observable $O$ are of\n",
        "interest. They are defined by\n",
        "\\begin{align}\n",
        "    \\langle O \\rangle = \\int dx \\, O(x) p(x),  \n",
        "\\end{align}\n",
        "where $x$ is the set of parameters (position, or momentum coordinates; spin orientations, $S_i$, in the Ising model) .  In most cases $x$ is a high dimensional object\n",
        "which makes classical numerical integration algorithms/summations impossible/cumbersome.\n",
        "Instead Monte-Carlo integration is employed and the integral  is approximated  by\n",
        "\\begin{align}\n",
        "    \\langle O \\rangle = \\frac1N \\sum_{i=1}^N O(x_i) \\pm \\sqrt{\\frac{{\\rm var}\\, (O)}{N}},  \n",
        "\\end{align}\n",
        "where the uncorrelated random numbers $x_i$ are sampled from the pdf $p(x)$. The problem is that we need to know the exact form of $p(x)$. Usually however we know only $q(x)$ given by \n",
        "\\begin{align}\n",
        "    p(x) = \\frac{q(x)}{Z}\\,, \n",
        "\\end{align}\n",
        "where the normalization $Z$ is unknown and is itself given by a similar integral: \n",
        "\\begin{align}\n",
        "    Z = \\int dx  \\,q(x)\\,. \n",
        "\\end{align}\n",
        "We thus need a method that would not require the knowledge of $Z$. The Metropolis algorithm was\n",
        "designed to solve precisely this problem.   \n",
        "\n",
        "Suppose we already have a sequence of parameters $x_0, ..., x_n$ which follow the pdf. We now add to the last element of this sequence a small perturbation $\\delta$ and set \n",
        "\\begin{align}\n",
        "    x'=x_n + \\delta\\,.\n",
        "\\end{align}\n",
        "In general all three quantities $x'$,$x_n$, and $\\delta$ are vectors. Similar to the rejection method we seek for a criterion which helps us to decide whether or not the\n",
        "test value $x'$  can be accepted as the next element of the sequence $x_0, ..., x_n$. The Metropolis  method proposes an acceptance probability of the form \n",
        "\\begin{align}\n",
        "    P (A| x', x_n) = {\\rm min} \\left (\\frac{p(x')}{p(x_n)}, 1 \\right)\\,.\n",
        "\\end{align}\n",
        "Hence if $ P (A| x', x_n) =1$ we set accept the test number, that is $x_{n+1} = x'$. If $ P (A| x', x_n) <1$ we draw a uniform random number $r\\in[0,1]$ and accept $x'$ if $r\\le P (A| x', x_n)$ and reject $x'$ otherwise. In this formulation the knowledge of the normalization factor $Z$ is no\n",
        "longer required since \n",
        "\\begin{align}\n",
        "    \\frac{p(x')}{p(x_n)} = \\frac{q(x')}{q(x_n)}\\,\n",
        "\\end{align}\n",
        "ant thus we can write \n",
        "\\begin{align}\n",
        "    P (A| x', x_n) = {\\rm min} \\left (\\frac{q(x')}{q(x_n)}, 1 \\right)\\,.\n",
        "\\end{align}\n",
        "A discussion of the underlying concepts and why the choice indeed\n",
        "samples random numbers according to the pdf $p(x)$ requires some knowledge\n",
        "of stochastics in general and of Markov-chains in particular. This goes beyond the course scope, but you can learn about it from the course textbook. \n",
        "\n",
        "It is important to mention that the Metropolis method satisfies the property of the detailed balance \n",
        "\\begin{align}\n",
        " P (A| x', x_n) p(x_n) = P (A| x_n, x') p(x')\\,.\n",
        "\\end{align}\n",
        "This can be shown explicitly. \n",
        "\n",
        "So far the question of how to choose the initialization point $x_0$ of the sequence\n",
        "stayed unanswered. This is clearly not a trivial problem and it is strongly related\n",
        "to one of the major disadvantages of the Metropolis algorithm, namely that\n",
        "subsequent random numbers  are strongly correlated. One of the most\n",
        "pragmatic approaches is to choose a starting point $x_0$ at random out of the parameter\n",
        "space and then discard it together with the first few members of the sequence.\n",
        "This approach is strongly motivated by a clear physical picture: The sequence of\n",
        "random numbers resembles the evolution of the physical system from an arbitrary\n",
        "initial point $x_0$ toward equilibrium which manifests itself in the condition of detailed\n",
        "balance. Hence, the approach of discarding the first few members of the sequence is referred to as *thermalization*. \n",
        "\n",
        "The integral of interest $\\langle O\\rangle$ is then approximated with the help of the second equation in this section, where the random numbers $x_k, x_{k+1}, \\dots, x_{k+N}$ are used, if the thermalization required $k$ steps. There is a remedy which helps to reduce correlations\n",
        "between subsequent random numbers within the sequence which is based on a\n",
        "similar strategy. In particular, the modified sequence\n",
        "\\begin{align}\n",
        "    x_k, x_{k+l}, x_{k+2l}, ... \\,\n",
        "\\end{align}\n",
        "generated by discarding $l$ intermediate random numbers will reduce correlations\n",
        "between the members of this final sequence of random numbers.\n",
        "\n",
        "**Problem 4**  Sample the normal distribution with $\\langle x \\rangle$ =0 and $\\sigma=1$ with the help of the Metropolis algorithm. Compare (plot!) the generated random numbers with the pdf in a histogram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5XbW4NYDyue"
      },
      "source": [
        "#Code"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}